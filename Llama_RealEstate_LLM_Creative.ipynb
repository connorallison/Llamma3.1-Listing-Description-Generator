{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fIN3jb5bMKJ"
      },
      "source": [
        "Load the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYmsW9UZuSuU",
        "outputId": "758c6123-5a92-44c8-cb86-eebc1f9e491d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 'housing_data_simulation.csv' already exists. Skipping upload.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "filename = \"housing_data_simulation2.csv\"\n",
        "\n",
        "# Check if the file already exists\n",
        "if not os.path.exists(filename):\n",
        "    # If it doesn't exist, upload it\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]  # Get the filename of the uploaded file\n",
        "    print(f\"File '{filename}' uploaded successfully.\")\n",
        "else:\n",
        "    print(f\"File '{filename}' already exists. Skipping upload.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BilsUEsjbOsr"
      },
      "source": [
        "If necessary, install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "IE8ivmlPX80T",
        "outputId": "344f93e7-ed22-4b91-dd1b-e5ff7e2ee871"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "479a464879ce43caba62fc8de9933d7c",
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA8SfHNrbkLu"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972,
          "referenced_widgets": [
            "6d3dc49de63e4c3c8641584a5f1e27ce",
            "be2682631380435eb02b026d919eb3af",
            "170b50b7e55b42bab4d8eef3419a7a92",
            "8da47393e58247e3acc1d87c4734ff96",
            "5fddf52dfeb24ef9a4aa37f7ecc52cb1",
            "bd9eb8b27a44440e977444f6de67ea99",
            "ce122c410a6544bb93ce9cce6dd0ff0c",
            "9440e42449774fb89f6775bd38d19827",
            "44954fb051dd4007a0d0dbda80964653",
            "1cd9e9adbee1426ba8553d626757f160",
            "87617299ad8d4b05a6de6dbb18fda8af"
          ]
        },
        "collapsed": true,
        "id": "Oo1fRjQekjmk",
        "outputId": "3ad4669c-4b0e-4ed0-d15e-6ad187a49c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using cuda device\n",
            "Loading base model: meta-llama/Llama-3.1-8B...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d3dc49de63e4c3c8641584a5f1e27ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "<ipython-input-2-e66376bac659>:160: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters:\n",
            "trainable params: 3,407,872 || all params: 8,033,710,080 || trainable%: 0.0424\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [350/350 39:15, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.123600</td>\n",
              "      <td>0.109914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.049500</td>\n",
              "      <td>0.052632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.043100</td>\n",
              "      <td>0.044340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training complete. Saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample Property:\n",
            "Price: $0.368600M, Bedrooms: 4, Bathrooms: 2.5, Square Footage: 1.7740K sq ft, Lot Size: 0.470 acres, Features: outdoor living spaces, custom lighting, designer windows, spa-like bathrooms, high-end appliances, smart home features, walk-in closets, gourmet kitchen, hardwood floors\n",
            "\n",
            "Actual Description:\n",
            "Welcome to your dream home! This breathtaking 4-bedroom residence redefines modern living. With 2.5 meticulously designed bathrooms and 1774 square feet of living space, every inch has been carefully considered. The 0.5-acre lot provides ample room for your lifestyle. Highlights include custom lighting, hardwood floors, walk-in closets. At $368,600.0, this home is a rare find you won't want to miss.\n",
            "\n",
            "Generated Description:\n",
            "Prepare to be impressed by this breathtaking 4-bedroom masterpiece. Anchored in a tree-lined street, the 1774 square foot home is a testament to sophisticated living. 2.5 luxurious bathrooms complement the expansive 0.5-acre property. Experience the extraordinary with features like designer windows, outdoor living spaces, luxury vinyl flooring, walk-in closets, spa-like bathrooms.\n",
            "\n",
            "Generated listing for test property:\n",
            "A home that truly has it all! This classic 3-bedroom property is a game-changer. Situated in an urban oasis, the 1800 square foot layout maximizes every inch of space. With 2 well-designed bathrooms and a generous lot size of 0.25 acres, comfort knows no bounds. Unique features such as Deck set this house apart from ordinary. An incredible value at $450,000.\n",
            "Saving the fine-tuned model...\n",
            "Model and tokenizer saved to /content/drive/MyDrive/ml_models/llama3_real_estate_model\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Your current token setup code is fine, just make sure it runs before your model code\n",
        "hf_token = userdata.get('llama3.1token')  # or use os.environ.get('HUGGINGFACE_TOKEN')\n",
        "os.environ['HUGGINGFACE_TOKEN'] = hf_token\n",
        "login(token=hf_token)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    LlamaTokenizer,\n",
        "    LlamaForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import bitsandbytes\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define a persistent storage location on your Google Drive\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/ml_models/llama3_real_estate_model\"\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Load and preprocess the data\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load and preprocess the dataset.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, encoding='utf-8')\n",
        "    except UnicodeDecodeError:\n",
        "        df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "\n",
        "    # Convert categorical features to strings\n",
        "    df['Bedrooms'] = df['Bedrooms'].astype(str)\n",
        "    df['Bathrooms'] = df['Bathrooms'].astype(str)\n",
        "\n",
        "    # Normalize numerical features\n",
        "    df['Price'] = df['Price'] / 1000000  # Convert to millions\n",
        "    df['Square Footage'] = df['Square Footage'] / 1000  # Convert to thousands\n",
        "    df['Lot Size (Acres)'] = df['Lot Size (Acres)']  # Keep as is\n",
        "\n",
        "    # Process features list into a string\n",
        "    if isinstance(df['Features'].iloc[0], str):\n",
        "        # If features are stored as strings that look like lists, convert them\n",
        "        try:\n",
        "            df['Features'] = df['Features'].apply(eval).apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
        "        except:\n",
        "            # If eval fails, assume it's already a string\n",
        "            pass\n",
        "    elif isinstance(df['Features'].iloc[0], list):\n",
        "        df['Features'] = df['Features'].apply(lambda x: ', '.join(x))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Custom dataset class for Llama\n",
        "class RealEstateLlamaDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.instructions = [\n",
        "            \"Write a compelling real estate listing description.\",\n",
        "            \"Describe this property in a persuasive and engaging way.\",\n",
        "            \"Craft an attractive and accurate listing for this home.\",\n",
        "            \"Generate a professional real estate description.\",\n",
        "            \"Provide a detailed and enticing listing for this property.\"\n",
        "        ]\n",
        "        self.styles = [\"<luxury>\", \"<cozy>\", \"<modern>\", \"<family>\", \"<urban>\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        property_text = f\"Price: ${row['Price']:.6f}M, Bedrooms: {row['Bedrooms']}, Bathrooms: {row['Bathrooms']}, \" \\\n",
        "                        f\"Square Footage: {row['Square Footage']:.4f}K sq ft, Lot Size: {row['Lot Size (Acres)']:.3f} acres, \" \\\n",
        "                        f\"Features: {row['Features']}\"\n",
        "\n",
        "        # Pick a random instruction prompt and style\n",
        "        instruction = random.choice(self.instructions)\n",
        "        style_token = random.choice(self.styles)\n",
        "\n",
        "        # You can optionally add a style token here, covered below\n",
        "        full_text = (\n",
        "            f\"<s>[INST] {style_token} {instruction} \"\n",
        "            f\"Make sure that the adjectives match the features. Price, Square Footage, and Lot Size must appear EXACTLY as input.\\n\\n\"\n",
        "            f\"{property_text} [/INST] {row['Listing Description']}</s>\"\n",
        "        )\n",
        "\n",
        "        encodings = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        labels = encodings['input_ids'].clone()\n",
        "        inst_tokens = self.tokenizer.encode(\" [/INST]\", add_special_tokens=False)\n",
        "        inst_end_positions = [i + len(inst_tokens) - 1 for i in range(len(labels[0]) - len(inst_tokens) + 1)\n",
        "                              if labels[0][i:i+len(inst_tokens)].tolist() == inst_tokens]\n",
        "        if inst_end_positions:\n",
        "            labels[0, :inst_end_positions[0]+1] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': encodings['input_ids'].squeeze(),\n",
        "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
        "            'labels': labels.squeeze(),\n",
        "            'raw_property': property_text,\n",
        "            'raw_description': row['Listing Description']\n",
        "        }\n",
        "\n",
        "\n",
        "# Training function using Trainer API\n",
        "def train_llama_model(model, train_dataset, val_dataset, tokenizer, output_dir, epochs=1, lr=2e-4, batch_size=1):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch\n",
        "        warmup_steps=50,  # Reduce warmup steps\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"steps\",  # Use eval_strategy instead of evaluation_strategy\n",
        "        eval_steps=100,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        save_total_limit=2,  # Keep only 2 checkpoints to save space\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"none\",\n",
        "        learning_rate=lr,\n",
        "        fp16=True,  # Use fp16 for training\n",
        "        optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
        "        max_grad_norm=0.3,  # Gradient clipping\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Training complete. Saving model...\")\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Generate a sample description\n",
        "def generate_sample(model, tokenizer, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    # Get a random sample\n",
        "    batch = next(iter(dataloader))\n",
        "    property_info = batch['raw_property'][0]\n",
        "    actual_description = batch['raw_description'][0]\n",
        "\n",
        "    print(\"\\nSample Property:\")\n",
        "    print(property_info)\n",
        "\n",
        "    print(\"\\nActual Description:\")\n",
        "    print(actual_description)\n",
        "\n",
        "    # Generate description\n",
        "    generated_text = generate_listing_description(model, tokenizer, property_info)\n",
        "\n",
        "    print(\"\\nGenerated Description:\")\n",
        "    print(generated_text)\n",
        "\n",
        "# Generate listing description\n",
        "def generate_listing_description(model, tokenizer, property_info, style_token=None, max_length=150):\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare input text with prompt in the instruction format\n",
        "    if style_token is None:\n",
        "        style_token = random.choice([\"<luxury>\",\"<cozy>\",\"<modern>\",\"<family>\",\"<urban>\"])\n",
        "    instruction = (f\"{style_token} Write a compelling real estate listing. \"\n",
        "                   f\"Price, Square Footage, Lot Size must match exactly.\")\n",
        "    input_text = f\"<s>[INST] {instruction}\\n\\n{property_info} [/INST]\"\n",
        "    tokenized_input = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_input[\"input_ids\"].to(device)\n",
        "    attention_mask = tokenized_input[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Generate text\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask = attention_mask,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=1.0,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.3,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decode the output\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
        "\n",
        "    description_part = generated_text\n",
        "\n",
        "    # First extract the part after [/INST] if present\n",
        "    if \"[/INST]\" in description_part:\n",
        "        description_part = description_part.split(\"[/INST]\")[1]\n",
        "\n",
        "    # Then remove any end tokens\n",
        "    end_tokens = [\"</s>\", \"<|end_of_text|>\"]\n",
        "    for token in end_tokens:\n",
        "        if token in description_part:\n",
        "            description_part = description_part.split(token)[0]\n",
        "\n",
        "    description_part = description_part.strip()\n",
        "\n",
        "    return description_part\n",
        "\n",
        "# Function to format a new property for prediction\n",
        "def format_property_input(property_dict):\n",
        "    price_in_millions = property_dict['Price'] / 1000000\n",
        "    sq_ft_in_thousands = property_dict['Square Footage'] / 1000\n",
        "\n",
        "    features = property_dict['Features']\n",
        "    if isinstance(features, list):\n",
        "        features = ', '.join(features)\n",
        "\n",
        "    property_text = f\"Price: ${price_in_millions:.6f}M, Bedrooms: {property_dict['Bedrooms']}, \" \\\n",
        "                     f\"Bathrooms: {property_dict['Bathrooms']}, \" \\\n",
        "                     f\"Square Footage: {sq_ft_in_thousands:.4f}K sq ft, \" \\\n",
        "                     f\"Lot Size: {property_dict['Lot Size (Acres)']:.3f} acres, \" \\\n",
        "                     f\"Features: {features}\"\n",
        "\n",
        "    return property_text\n",
        "\n",
        "# Setup LoRA for parameter-efficient fine-tuning\n",
        "def setup_lora_model(base_model_path=\"meta-llama/Llama-3.1-8B\", hf_token=None):\n",
        "    print(f\"Loading base model: {base_model_path}...\")\n",
        "\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        base_model_path,\n",
        "        token=hf_token,\n",
        "        use_fast=True\n",
        "    )\n",
        "\n",
        "    style_tokens = [\"<luxury>\", \"<cozy>\", \"<modern>\", \"<family>\", \"<urban>\"]\n",
        "    tokenizer.add_special_tokens({'additional_special_tokens': style_tokens})\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "    # Configure 4-bit quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "\n",
        "    # Load the model with 4-bit quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_path,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        token=hf_token\n",
        "    )\n",
        "\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Define LoRA configuration\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        inference_mode=False,\n",
        "        r=8,  # Lower rank for less memory\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"]  # Optimize for memory: target fewer modules\n",
        "    )\n",
        "\n",
        "    # Get the PEFT model\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Print trainable parameters\n",
        "    print(\"Trainable parameters:\")\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Main execution\n",
        "def main(file_path, output_dir=\"./llama3_real_estate_model\", batch_size=4, epochs=3, model_path=\"meta-llama/Llama-3.1-8B\", hf_token=None):\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load data\n",
        "    df = load_data(file_path)\n",
        "\n",
        "    # Split the data\n",
        "    train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Initialize model and tokenizer with LoRA for efficient fine-tuning\n",
        "    model, tokenizer = setup_lora_model(model_path, hf_token)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = RealEstateLlamaDataset(train_df, tokenizer)\n",
        "    val_dataset = RealEstateLlamaDataset(val_df, tokenizer)\n",
        "\n",
        "    # Train model\n",
        "    trained_model = train_llama_model(\n",
        "        model,\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        output_dir,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Create a small dataloader for generating samples\n",
        "    sample_dataloader = DataLoader(val_dataset, batch_size=1)\n",
        "\n",
        "    # Generate a sample\n",
        "    generate_sample(trained_model, tokenizer, sample_dataloader)\n",
        "\n",
        "    # Test with a new property\n",
        "    test_property = {\n",
        "        'Price': 450000,\n",
        "        'Bedrooms': '3',\n",
        "        'Bathrooms': '2',\n",
        "        'Square Footage': 1800,\n",
        "        'Lot Size (Acres)': 0.25,\n",
        "        'Features': ['Hardwood floors', 'Updated kitchen', 'Finished basement', 'Deck']\n",
        "    }\n",
        "\n",
        "    property_text = format_property_input(test_property)\n",
        "    description = generate_listing_description(trained_model, tokenizer, property_text)\n",
        "\n",
        "    print(\"\\nGenerated listing for test property:\")\n",
        "    print(description)\n",
        "\n",
        "    # Save the trained model\n",
        "    print(\"Saving the fine-tuned model...\")\n",
        "\n",
        "    # Save the LoRA adapter weights\n",
        "    trained_model.save_pretrained(f\"{output_dir}/lora_adapter\")\n",
        "\n",
        "    # Save the tokenizer\n",
        "    tokenizer.save_pretrained(f\"{output_dir}/tokenizer\")\n",
        "\n",
        "    print(f\"Model and tokenizer saved to {output_dir}\")\n",
        "\n",
        "#hf_token = os.environ.get('HUGGINGFACE_TOKEN', 'COPY TOKEN HERE')\n",
        "\n",
        "main(\n",
        "    file_path=filename,\n",
        "    output_dir = MODEL_SAVE_PATH,\n",
        "    batch_size=1,\n",
        "    epochs=2,\n",
        "    model_path=\"meta-llama/Llama-3.1-8B\",\n",
        "    hf_token=hf_token\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP8Z_gjuktlY"
      },
      "source": [
        "Import test file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz67bU8bVjRu",
        "outputId": "bcaffa7c-b55c-4bee-818e-cab92d0cc236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 'housing_data_simulation test copy.csv' already exists. Skipping upload.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "filename = \"housing_data_simulation test copy.csv\"\n",
        "\n",
        "# Check if the file already exists\n",
        "if not os.path.exists(filename):\n",
        "    # If it doesn't exist, upload it\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]  # Get the filename of the uploaded file\n",
        "    print(f\"File '{filename}' uploaded successfully.\")\n",
        "else:\n",
        "    print(f\"File '{filename}' already exists. Skipping upload.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UQ5Gq36G2Yt"
      },
      "source": [
        "Test the model on 50 observations for later annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595,
          "referenced_widgets": [
            "0dd291979b9d4de28cb6ef2a4049496c",
            "d3442c2ce3fa4a9aa4ac0c3c0605076d",
            "a1099c84fa064b7b95f819997ede1a80",
            "bf7a80fd13e7416aa1135a7fada6351e",
            "9cf6e7e046d14ffa8c0bbaca6da36d44",
            "b21d60f174504342932845ed8eaeeddd",
            "d58b77591e484f2aaf1cae43f099fadf",
            "233c445c105145ed97403fca166c0505",
            "b522f0b455e141a59240996309cb76a8",
            "297c9863fc164463a33001b5a1af8932",
            "4ac1a0cd57484d158265c8fbce76e2a6"
          ]
        },
        "id": "siHQ5PTp5bOg",
        "outputId": "cd3e7397-5a15-45c9-814d-47e358b6fbed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dd291979b9d4de28cb6ef2a4049496c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            " 10%|█         | 5/50 [00:39<05:48,  7.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 5/50 descriptions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 10/50 [01:11<04:06,  6.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 10/50 descriptions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 15/50 [01:51<04:40,  8.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 15/50 descriptions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 20/50 [02:26<03:42,  7.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 20/50 descriptions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 25/50 [02:57<02:31,  6.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 25/50 descriptions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 30/50 [03:31<02:11,  6.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 30/50 descriptions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 33/50 [03:53<02:00,  7.08s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6b860e30a5f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;31m# Generate descriptions for all properties in the CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     generate_batch_descriptions(\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0minput_csv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0moutput_csv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6b860e30a5f5>\u001b[0m in \u001b[0;36mgenerate_batch_descriptions\u001b[0;34m(input_csv_path, output_csv_path, model_path, max_length, save_interval)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mstyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTYLE_TOKENS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0minstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINSTRUCTIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_listing_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0mdf_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Generated Listing Description'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6b860e30a5f5>\u001b[0m in \u001b[0;36mgenerate_listing_description\u001b[0;34m(model, property_info, style_token, instruction, max_tokens)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             output = model.generate(\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3434\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3436\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    822\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m                 )\n\u001b[1;32m    570\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    572\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mhidden_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mrequires_conversion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM, LlamaTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "def post_process_description(generated_text, property_info):\n",
        "    \"\"\"\n",
        "    Ensures that numerical values in the generated text match the desired format:\n",
        "    - Price: $300,000 (full dollar amount with commas)\n",
        "    - Square footage: 1234 (whole number)\n",
        "    - Acres: 0.15 (decimal number)\n",
        "\n",
        "    Args:\n",
        "        generated_text (str): The generated listing description\n",
        "        property_info (str): The original property information string\n",
        "\n",
        "    Returns:\n",
        "        str: The processed description with correctly formatted numerical values\n",
        "    \"\"\"\n",
        "    # Extract the original values from property_info\n",
        "    price_match = re.search(r'Price: \\$([\\d.]+)M', property_info)\n",
        "    sqft_match = re.search(r'Square Footage: ([\\d.]+)K', property_info)\n",
        "    lot_match = re.search(r'Lot Size: ([\\d.]+) acres', property_info)\n",
        "\n",
        "    # Convert to the desired formats\n",
        "    if price_match:\n",
        "        # Convert $X.YM to $X,YY0,000\n",
        "        price_in_millions = float(price_match.group(1))\n",
        "        price_in_dollars = int(price_in_millions * 1000000)\n",
        "        formatted_price = \"${:,}\".format(price_in_dollars)\n",
        "    else:\n",
        "        formatted_price = None\n",
        "\n",
        "    if sqft_match:\n",
        "        # Convert X.YK to whole number\n",
        "        sqft_in_thousands = float(sqft_match.group(1))\n",
        "        formatted_sqft = str(int(sqft_in_thousands * 1000))\n",
        "    else:\n",
        "        formatted_sqft = None\n",
        "\n",
        "    if lot_match:\n",
        "        # Keep exact decimal format for acreage\n",
        "        formatted_lot = lot_match.group(1)\n",
        "    else:\n",
        "        formatted_lot = None\n",
        "\n",
        "    # Only proceed with replacements if we have valid values\n",
        "    if not (formatted_price or formatted_sqft or formatted_lot):\n",
        "        print(\"Warning: Could not extract values from property_info\")\n",
        "        return generated_text\n",
        "\n",
        "    processed_text = generated_text\n",
        "\n",
        "    # Replace price variations\n",
        "    if formatted_price:\n",
        "        # Handle $X.YM format\n",
        "        processed_text = re.sub(r'\\$([\\d,.]+)M', formatted_price, processed_text)\n",
        "        processed_text = re.sub(r'\\$([\\d,.]+) million', formatted_price, processed_text)\n",
        "        processed_text = re.sub(r'\\$([\\d,.]+) Million', formatted_price, processed_text)\n",
        "\n",
        "        # Handle normal dollar amounts\n",
        "        processed_text = re.sub(r'\\$([\\d,]+)', formatted_price, processed_text)\n",
        "\n",
        "        # Handle spelled out numbers\n",
        "        million_words = ['million', 'Million']\n",
        "        for word in million_words:\n",
        "            pattern = rf'(\\$[\\d,.]+ {word}|\\$[\\d,.]+{word})'\n",
        "            processed_text = re.sub(pattern, formatted_price, processed_text)\n",
        "\n",
        "    # Replace square footage variations\n",
        "    if formatted_sqft:\n",
        "        # Handle XK sq ft format\n",
        "        processed_text = re.sub(r'([\\d,.]+)K sq ft', formatted_sqft + \" sq ft\", processed_text)\n",
        "        processed_text = re.sub(r'([\\d,.]+)K square feet', formatted_sqft + \" square feet\", processed_text)\n",
        "\n",
        "        # Handle normal square footage formats\n",
        "        processed_text = re.sub(r'([\\d,.]+) sq\\. ft\\.', formatted_sqft + \" sq. ft.\", processed_text)\n",
        "        processed_text = re.sub(r'([\\d,.]+) square feet', formatted_sqft + \" square feet\", processed_text)\n",
        "        processed_text = re.sub(r'([\\d,.]+) sf', formatted_sqft + \" sq ft\", processed_text)\n",
        "\n",
        "        # Handle thousand variations\n",
        "        thousand_words = ['thousand square feet', 'thousand sq ft', 'thousand sq. ft.']\n",
        "        for word in thousand_words:\n",
        "            pattern = rf'([\\d,.]+) {word}'\n",
        "            processed_text = re.sub(pattern, formatted_sqft + \" square feet\", processed_text)\n",
        "\n",
        "    # Replace lot size variations\n",
        "    if formatted_lot:\n",
        "        # Standard lot size formats\n",
        "        processed_text = re.sub(r'([\\d,.]+) acre(?!s)', formatted_lot + \" acre\", processed_text)\n",
        "        processed_text = re.sub(r'([\\d,.]+) acres', formatted_lot + \" acres\", processed_text)\n",
        "        processed_text = re.sub(r'([\\d,.]+)-acre', formatted_lot + \"-acre\", processed_text)\n",
        "\n",
        "    # Check if values were properly inserted and add them if missing\n",
        "    if formatted_price and formatted_price not in processed_text:\n",
        "        processed_text = f\"Priced at {formatted_price}. \" + processed_text\n",
        "\n",
        "    if formatted_sqft and formatted_sqft + \" sq\" not in processed_text:\n",
        "        processed_text = f\"This {formatted_sqft} sq ft home \" + processed_text\n",
        "\n",
        "    if formatted_lot and formatted_lot + \" acre\" not in processed_text:\n",
        "        if \"acre\" in processed_text:\n",
        "            # Try to insert near an existing mention of acres\n",
        "            processed_text = re.sub(r'([\\d,.]+) acres?', f\"{formatted_lot} acres\", processed_text, count=1)\n",
        "        else:\n",
        "            # Add to beginning if no mention exists\n",
        "            processed_text = f\"Situated on {formatted_lot} acres. \" + processed_text\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "# Define style tokens and instruction prompts\n",
        "STYLE_TOKENS = [\"<luxury>\", \"<cozy>\", \"<modern>\", \"<family>\", \"<urban>\"]\n",
        "INSTRUCTIONS = [\n",
        "    \"Write a compelling real estate listing description.\",\n",
        "    \"Describe this property in a persuasive and engaging way.\",\n",
        "    \"Craft an attractive and accurate listing for this home.\",\n",
        "    \"Generate a professional real estate description.\",\n",
        "    \"Provide a detailed and enticing listing for this property.\"\n",
        "]\n",
        "\n",
        "\n",
        "def generate_batch_descriptions(\n",
        "    input_csv_path,\n",
        "    output_csv_path,\n",
        "    model_path=\"./llama3_real_estate_model\",\n",
        "    max_length=150,\n",
        "    save_interval=5\n",
        "):\n",
        "    # Load data\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "    if 'Generated Listing Description' not in df.columns:\n",
        "        df['Generated Listing Description'] = None\n",
        "\n",
        "    # Prepare tokenizer and ensure it matches training vocab\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    # Add style tokens (must match training)\n",
        "    tokenizer.add_special_tokens({'additional_special_tokens': STYLE_TOKENS})\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def format_property_input(row):\n",
        "        price_m = float(row['Price']) / 1_000_000 if row.get('Price') else 0\n",
        "        sqft_k = float(row['Square Footage']) / 1000 if row.get('Square Footage') else 0\n",
        "        lot = float(row['Lot Size (Acres)']) if row.get('Lot Size (Acres)') else 0\n",
        "        features = row.get('Features', \"None\")\n",
        "        if pd.isna(features): features = \"None\"\n",
        "        return (\n",
        "            f\"Price: ${price_m:.6f}M, Bedrooms: {row.get('Bedrooms','0')}, \"\n",
        "            f\"Bathrooms: {row.get('Bathrooms','0')}, \"\n",
        "            f\"Square Footage: {sqft_k:.4f}K sq ft, Lot Size: {lot:.3f} acres, Features: {features}\"\n",
        "        )\n",
        "\n",
        "    def load_model():\n",
        "        gc.collect(); torch.cuda.empty_cache()\n",
        "        peft_config_path = os.path.join(model_path, \"adapter_config.json\")\n",
        "        is_peft = os.path.exists(peft_config_path)\n",
        "\n",
        "        if is_peft:\n",
        "            # Load base model\n",
        "            peft_cfg = PeftConfig.from_pretrained(model_path)\n",
        "            base_model = LlamaForCausalLM.from_pretrained(\n",
        "                peft_cfg.base_model_name_or_path,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.float16,\n",
        "                load_in_4bit=True,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "            # Resize embeddings to include style tokens\n",
        "            base_model.resize_token_embeddings(len(tokenizer))\n",
        "            # Load PEFT adapter weights\n",
        "            model = PeftModel.from_pretrained(\n",
        "                base_model,\n",
        "                model_path,\n",
        "                torch_dtype=torch.float16\n",
        "            )\n",
        "        else:\n",
        "            # Load full model\n",
        "            model = LlamaForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.float16,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "            # Resize embeddings in case tokenizer vocab differs\n",
        "            model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def generate_listing_description(model, property_info, style_token, instruction, max_tokens=max_length):\n",
        "        # Extract numeric values\n",
        "        price_m = re.search(r'Price: \\$(?P<p>[\\d.]+)M', property_info)\n",
        "        sqft_k = re.search(r'Square Footage: (?P<s>[\\d.]+)K', property_info)\n",
        "        lot = re.search(r'Lot Size: (?P<l>[\\d.]+) acres', property_info)\n",
        "\n",
        "        # Format for prompt\n",
        "        price_val = f\"${int(float(price_m.group('p'))*1e6):,}\" if price_m else \"unknown price\"\n",
        "        sqft_val = f\"{int(float(sqft_k.group('s'))*1000)}\" if sqft_k else \"unknown sqft\"\n",
        "        lot_val = lot.group('l') if lot else \"unknown acreage\"\n",
        "\n",
        "        prompt = (\n",
        "            f\"<s>[INST] {style_token} {instruction} Make sure adjectives match features. \"\n",
        "            f\"Price: {price_val}, Square Footage: {sqft_val} sq ft, Lot Size: {lot_val} acres.\\n\\n\"\n",
        "            f\"{property_info} [/INST]\"\n",
        "        )\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                temperature=1.0,\n",
        "                top_p=0.95,\n",
        "                repetition_penalty=1.3,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
        "        if \"[/INST]\" in text:\n",
        "            text = text.split(\"[/INST]\")[1]\n",
        "        for tok in [\"</s>\", \"<|end_of_text|>\"]:\n",
        "            text = text.split(tok)[0]\n",
        "        return post_process_description(text.strip(), property_info)\n",
        "\n",
        "    # Main generation loop\n",
        "    df_out = df.copy()\n",
        "    remaining = df_out.index[df_out['Generated Listing Description'].isna()].tolist()\n",
        "    if remaining:\n",
        "        model = load_model()\n",
        "        for i, idx in enumerate(tqdm(remaining), 1):\n",
        "            row = df_out.loc[idx]\n",
        "            property_text = format_property_input(row)\n",
        "            style = random.choice(STYLE_TOKENS)\n",
        "            instr = random.choice(INSTRUCTIONS)\n",
        "            desc = generate_listing_description(model, property_text, style, instr)\n",
        "            df_out.at[idx, 'Generated Listing Description'] = desc\n",
        "            if i % save_interval == 0 or i == len(remaining):\n",
        "                df_out.to_csv(output_csv_path, index=False)\n",
        "                print(f\"Saved {i}/{len(remaining)} descriptions\")\n",
        "        del model; torch.cuda.empty_cache(); gc.collect()\n",
        "    else:\n",
        "        print(\"No properties to process.\")\n",
        "    return df_out\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Update these paths as needed\n",
        "    input_csv = \"/content/housing_data_simulation test copy.csv\"  # Your input CSV file\n",
        "    output_csv = \"housing_data_with_descriptions.csv\"  # Where to save the results\n",
        "    model_dir = \"/content/drive/MyDrive/ml_models/llama3_real_estate_model\"  # Directory containing your trained model\n",
        "\n",
        "    # Generate descriptions for all properties in the CSV\n",
        "    generate_batch_descriptions(\n",
        "        input_csv_path=input_csv,\n",
        "        output_csv_path=output_csv,\n",
        "        model_path=model_dir,\n",
        "        max_length=150,\n",
        "        save_interval=5  # Save after each property to prevent data loss\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3eyHsUtKTJR"
      },
      "source": [
        "Potentially more creative?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxaGYWMQJ-op"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Expanded style tokens and instruction prompts for greater variety\n",
        "STYLE_TOKENS = [\n",
        "    \"<luxury>\", \"<cozy>\", \"<modern>\", \"<family>\", \"<urban>\",\n",
        "    \"<rustic>\", \"<minimalist>\", \"<elegant>\", \"<eclectic>\", \"<charming>\"\n",
        "]\n",
        "INSTRUCTIONS = [\n",
        "    \"Write a compelling real estate listing description.\",\n",
        "    \"Describe this property in a persuasive and engaging way.\",\n",
        "    \"Craft an attractive and accurate listing for this home.\",\n",
        "    \"Generate a professional real estate description.\",\n",
        "    \"Provide a detailed and enticing listing for this property.\",\n",
        "    \"Capture the unique charm and features of this home.\",\n",
        "    \"Highlight the best selling points of this property in an engaging narrative.\",\n",
        "    \"Create a vivid and inviting description for potential buyers.\",\n",
        "    \"Compose a descriptive and energetic real estate listing.\",\n",
        "    \"Frame this home in a lifestyle-oriented and enticing way.\"\n",
        "]\n",
        "\n",
        "# Sampling configuration ranges for diversity\n",
        "SAMPLING_CONFIG = {\n",
        "    \"temperature\": (0.7, 1.2),\n",
        "    \"top_p\": (0.8, 0.98),\n",
        "    \"repetition_penalty\": (1.0, 1.4)\n",
        "}\n",
        "\n",
        "\n",
        "def post_process_description(generated_text, property_info):\n",
        "    # (unchanged post-processing logic...)\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "def generate_batch_descriptions(\n",
        "    input_csv_path,\n",
        "    output_csv_path,\n",
        "    model_path=\"./llama3_real_estate_model\",\n",
        "    max_length=150,\n",
        "    save_interval=5\n",
        "):\n",
        "    # Load data\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "    df.setdefault('Generated Listing Description', None)\n",
        "\n",
        "    # Tokenizer setup\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    tokenizer.add_special_tokens({'additional_special_tokens': STYLE_TOKENS})\n",
        "    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def format_property_input(row):\n",
        "        price_m = float(row.get('Price', 0)) / 1_000_000\n",
        "        sqft_k = float(row.get('Square Footage', 0)) / 1000\n",
        "        lot = float(row.get('Lot Size (Acres)', 0))\n",
        "        features = row.get('Features', \"None\")\n",
        "        if pd.isna(features): features = \"None\"\n",
        "        return (\n",
        "            f\"Price: ${price_m:.6f}M, Bedrooms: {row.get('Bedrooms','0')}, \"\n",
        "            f\"Bathrooms: {row.get('Bathrooms','0')}, \"\n",
        "            f\"Square Footage: {sqft_k:.4f}K sq ft, Lot Size: {lot:.3f} acres, Features: {features}\"\n",
        "        )\n",
        "\n",
        "    def load_model():\n",
        "        # Clear memory\n",
        "        gc.collect(); torch.cuda.empty_cache()\n",
        "        peft_cfg_path = os.path.join(model_path, \"adapter_config.json\")\n",
        "        is_peft = os.path.exists(peft_cfg_path)\n",
        "        if is_peft:\n",
        "            peft_cfg = PeftConfig.from_pretrained(model_path)\n",
        "            base = LlamaForCausalLM.from_pretrained(\n",
        "                peft_cfg.base_model_name_or_path,\n",
        "                device_map=\"auto\", torch_dtype=torch.float16,\n",
        "                load_in_4bit=True, low_cpu_mem_usage=True\n",
        "            )\n",
        "            base.resize_token_embeddings(len(tokenizer))\n",
        "            model = PeftModel.from_pretrained(base, model_path, torch_dtype=torch.float16)\n",
        "        else:\n",
        "            model = LlamaForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                device_map=\"auto\", torch_dtype=torch.float16,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "            model.resize_token_embeddings(len(tokenizer))\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def generate_listing_description(model, property_info, style_token, instruction):\n",
        "        # Randomize sampling parameters\n",
        "        temp = random.uniform(*SAMPLING_CONFIG['temperature'])\n",
        "        top_p = random.uniform(*SAMPLING_CONFIG['top_p'])\n",
        "        rep_pen = random.uniform(*SAMPLING_CONFIG['repetition_penalty'])\n",
        "\n",
        "        # Extract and format numeric constraints\n",
        "        price_m = re.search(r'Price: \\$(?P<p>[\\d.]+)M', property_info)\n",
        "        sqft_k = re.search(r'Square Footage: (?P<s>[\\d.]+)K', property_info)\n",
        "        lot = re.search(r'Lot Size: (?P<l>[\\d.]+) acres', property_info)\n",
        "        price_val = f\"${int(float(price_m.group('p'))*1e6):,}\" if price_m else \"\"\n",
        "        sqft_val = f\"{int(float(sqft_k.group('s'))*1000)}\" if sqft_k else \"\"\n",
        "        lot_val = lot.group('l') if lot else \"\"\n",
        "\n",
        "        prompt = (\n",
        "            f\"<s>[INST] {style_token} {instruction} \"\n",
        "            f\"Include exact values: Price: {price_val}, Square Footage: {sqft_val} sq ft, Lot Size: {lot_val} acres.\\n\\n\"\n",
        "            f\"{property_info} [/INST]\"\n",
        "        )\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_length,\n",
        "                temperature=temp,\n",
        "                top_p=top_p,\n",
        "                repetition_penalty=rep_pen,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
        "        if \"[/INST]\" in text:\n",
        "            text = text.split(\"[/INST]\")[1]\n",
        "        for tok in [\"</s>\", \"<|end_of_text|>\"]:\n",
        "            text = text.split(tok)[0]\n",
        "        return post_process_description(text.strip(), property_info)\n",
        "\n",
        "    # Generation loop\n",
        "    df_out = df.copy()\n",
        "    to_process = df_out[df_out['Generated Listing Description'].isna()].index.tolist()\n",
        "    if not to_process:\n",
        "        print(\"No new properties to process.\")\n",
        "        return df_out\n",
        "\n",
        "    model = load_model()\n",
        "    for idx in tqdm(to_process, desc=\"Generating listings\"):\n",
        "        row = df_out.loc[idx]\n",
        "        prop_text = format_property_input(row)\n",
        "        style = random.choice(STYLE_TOKENS)\n",
        "        instr = random.choice(INSTRUCTIONS)\n",
        "        desc = generate_listing_description(model, prop_text, style, instr)\n",
        "        df_out.at[idx, 'Generated Listing Description'] = desc\n",
        "        if (to_process.index(idx)+1) % save_interval == 0:\n",
        "            df_out.to_csv(output_csv_path, index=False)\n",
        "    # Final save\n",
        "    df_out.to_csv(output_csv_path, index=False)\n",
        "    # Cleanup\n",
        "    del model; torch.cuda.empty_cache(); gc.collect()\n",
        "    return df_out\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_batch_descriptions(\n",
        "        input_csv_path=\"properties.csv\",\n",
        "        output_csv_path=\"output.csv\",\n",
        "        model_path=\"./llama3_real_estate_model\",\n",
        "        max_length=150,\n",
        "        save_interval=5\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCSieJALZ-gz"
      },
      "source": [
        "We could use the evaluations to train a model to rate listing descriptions. That way you could produce several but just return the highest rated one."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dd291979b9d4de28cb6ef2a4049496c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3442c2ce3fa4a9aa4ac0c3c0605076d",
              "IPY_MODEL_a1099c84fa064b7b95f819997ede1a80",
              "IPY_MODEL_bf7a80fd13e7416aa1135a7fada6351e"
            ],
            "layout": "IPY_MODEL_9cf6e7e046d14ffa8c0bbaca6da36d44"
          }
        },
        "170b50b7e55b42bab4d8eef3419a7a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9440e42449774fb89f6775bd38d19827",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44954fb051dd4007a0d0dbda80964653",
            "value": 4
          }
        },
        "1cd9e9adbee1426ba8553d626757f160": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "233c445c105145ed97403fca166c0505": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "297c9863fc164463a33001b5a1af8932": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44954fb051dd4007a0d0dbda80964653": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ac1a0cd57484d158265c8fbce76e2a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fddf52dfeb24ef9a4aa37f7ecc52cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d3dc49de63e4c3c8641584a5f1e27ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be2682631380435eb02b026d919eb3af",
              "IPY_MODEL_170b50b7e55b42bab4d8eef3419a7a92",
              "IPY_MODEL_8da47393e58247e3acc1d87c4734ff96"
            ],
            "layout": "IPY_MODEL_5fddf52dfeb24ef9a4aa37f7ecc52cb1"
          }
        },
        "87617299ad8d4b05a6de6dbb18fda8af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8da47393e58247e3acc1d87c4734ff96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cd9e9adbee1426ba8553d626757f160",
            "placeholder": "​",
            "style": "IPY_MODEL_87617299ad8d4b05a6de6dbb18fda8af",
            "value": " 4/4 [01:17&lt;00:00, 16.41s/it]"
          }
        },
        "9440e42449774fb89f6775bd38d19827": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cf6e7e046d14ffa8c0bbaca6da36d44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1099c84fa064b7b95f819997ede1a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_233c445c105145ed97403fca166c0505",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b522f0b455e141a59240996309cb76a8",
            "value": 4
          }
        },
        "b21d60f174504342932845ed8eaeeddd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b522f0b455e141a59240996309cb76a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd9eb8b27a44440e977444f6de67ea99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be2682631380435eb02b026d919eb3af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd9eb8b27a44440e977444f6de67ea99",
            "placeholder": "​",
            "style": "IPY_MODEL_ce122c410a6544bb93ce9cce6dd0ff0c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "bf7a80fd13e7416aa1135a7fada6351e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_297c9863fc164463a33001b5a1af8932",
            "placeholder": "​",
            "style": "IPY_MODEL_4ac1a0cd57484d158265c8fbce76e2a6",
            "value": " 4/4 [01:34&lt;00:00, 20.43s/it]"
          }
        },
        "ce122c410a6544bb93ce9cce6dd0ff0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3442c2ce3fa4a9aa4ac0c3c0605076d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b21d60f174504342932845ed8eaeeddd",
            "placeholder": "​",
            "style": "IPY_MODEL_d58b77591e484f2aaf1cae43f099fadf",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d58b77591e484f2aaf1cae43f099fadf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

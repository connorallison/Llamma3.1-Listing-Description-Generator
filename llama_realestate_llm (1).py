# -*- coding: utf-8 -*-
"""Llama_RealEstate_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ubA6J1SPNw0SwdgH1xwAPRRqS0zXuQDL

Load the training data
"""

from google.colab import files
import os

filename = "housing_data_simulation.csv"

# Check if the file already exists
if not os.path.exists(filename):
    # If it doesn't exist, upload it
    uploaded = files.upload()
    filename = list(uploaded.keys())[0]  # Get the filename of the uploaded file
    print(f"File '{filename}' uploaded successfully.")
else:
    print(f"File '{filename}' already exists. Skipping upload.")

"""If necessary, install bitsandbytes"""

!pip install bitsandbytes

"""Train the model"""

import os
from huggingface_hub import login
from google.colab import userdata
userdata.get('llama3.1token')

# Your current token setup code is fine, just make sure it runs before your model code
hf_token = userdata.get('llama3.1token')  # or use os.environ.get('HUGGINGFACE_TOKEN')
os.environ['HUGGINGFACE_TOKEN'] = hf_token
login(token=hf_token)

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    LlamaTokenizer,
    LlamaForCausalLM,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from sklearn.model_selection import train_test_split
import random
from tqdm import tqdm
import os
from peft import get_peft_model, LoraConfig, TaskType
import bitsandbytes


# Set random seeds for reproducibility
seed_val = 42
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed_val)

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using {device} device")

# Load and preprocess the data
def load_data(file_path):
    """Load and preprocess the dataset."""
    df = pd.read_csv(file_path)

    # Convert categorical features to strings
    df['Bedrooms'] = df['Bedrooms'].astype(str)
    df['Bathrooms'] = df['Bathrooms'].astype(str)

    # Normalize numerical features
    df['Price'] = df['Price'] / 1000000  # Convert to millions
    df['Square Footage'] = df['Square Footage'] / 1000  # Convert to thousands
    df['Lot Size (Acres)'] = df['Lot Size (Acres)']  # Keep as is

    # Process features list into a string
    if isinstance(df['Features'].iloc[0], str):
        # If features are stored as strings that look like lists, convert them
        try:
            df['Features'] = df['Features'].apply(eval).apply(lambda x: ', '.join(x) if isinstance(x, list) else x)
        except:
            # If eval fails, assume it's already a string
            pass
    elif isinstance(df['Features'].iloc[0], list):
        df['Features'] = df['Features'].apply(lambda x: ', '.join(x))

    return df

# Custom dataset class for Llama
class RealEstateLlamaDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]

        # Create input text with a clear instruction for the model
        property_text = f"Price: ${row['Price']:.2f}M, Bedrooms: {row['Bedrooms']}, Bathrooms: {row['Bathrooms']}, " \
                         f"Square Footage: {row['Square Footage']:.1f}K sq ft, Lot Size: {row['Lot Size (Acres)']:.2f} acres, " \
                         f"Features: {row['Features']}"

        # Create full prompt in the instruction format Llama understands
        full_text = f"<s>[INST] Write a compelling and accurate real estate listing description for this property: \n\n{property_text} [/INST] {row['Listing Description']}</s>"

        # Tokenize full text
        encodings = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )

        # Create labels - set to -100 for the instruction part so it's not included in loss calculation
        labels = encodings['input_ids'].clone()

        # Find where the [/INST] token ends
        inst_tokens = self.tokenizer.encode(" [/INST]", add_special_tokens=False)
        inst_end_positions = []

        for i in range(len(labels[0]) - len(inst_tokens) + 1):
            if labels[0][i:i+len(inst_tokens)].tolist() == inst_tokens:
                inst_end_positions.append(i + len(inst_tokens) - 1)

        if inst_end_positions:
            # Set all tokens before the end of [/INST] to -100
            labels[0, :inst_end_positions[0]+1] = -100

        return {
            'input_ids': encodings['input_ids'].squeeze(),
            'attention_mask': encodings['attention_mask'].squeeze(),
            'labels': labels.squeeze(),
            'raw_property': property_text,
            'raw_description': row['Listing Description']
        }

# Training function using Trainer API
def train_llama_model(model, train_dataset, val_dataset, tokenizer, output_dir, epochs=1, lr=2e-4, batch_size=1):
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch
        warmup_steps=50,  # Reduce warmup steps
        weight_decay=0.01,
        logging_dir=f"{output_dir}/logs",
        logging_steps=10,
        eval_strategy="steps",  # Use eval_strategy instead of evaluation_strategy
        eval_steps=100,
        save_strategy="steps",
        save_steps=100,
        save_total_limit=2,  # Keep only 2 checkpoints to save space
        load_best_model_at_end=True,
        report_to="none",
        learning_rate=lr,
        fp16=True,  # Use fp16 for training
        optim="paged_adamw_8bit",  # Memory-efficient optimizer
        max_grad_norm=0.3,  # Gradient clipping
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
    )

    print("Starting training...")
    trainer.train()

    print("Training complete. Saving model...")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)

    return model

# Generate a sample description
def generate_sample(model, tokenizer, dataloader):
    model.eval()

    # Get a random sample
    batch = next(iter(dataloader))
    property_info = batch['raw_property'][0]
    actual_description = batch['raw_description'][0]

    print("\nSample Property:")
    print(property_info)

    print("\nActual Description:")
    print(actual_description)

    # Generate description
    generated_text = generate_listing_description(model, tokenizer, property_info)

    print("\nGenerated Description:")
    print(generated_text)

# Generate listing description
def generate_listing_description(model, tokenizer, property_info, max_length=150):
    model.eval()

    # Prepare input text with prompt in the instruction format
    input_text = f"<s>[INST] Write a compelling and accurate real estate listing description for this property. You MUST include and EXACTLY match these numeric values without rounding or changing them: Price: ${property_info.split('Price: $')[1].split('M')[0]}M, Square Footage: {property_info.split('Square Footage: ')[1].split('K')[0]}K sq ft, Lot Size: {property_info.split('Lot Size: ')[1].split(' acres')[0]} acres. \n\n{property_info} [/INST]"
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)

    # Generate text
    with torch.no_grad():
        output = model.generate(
            input_ids=input_ids,
            max_new_tokens=max_length,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.2,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Decode the output
    generated_text = tokenizer.decode(output[0], skip_special_tokens=False)

    # Extract just the generated description by finding everything after [/INST]
    description_part = generated_text.split("[/INST]")[1].split("</s>")[0].strip()

    return description_part

# Function to format a new property for prediction
def format_property_input(property_dict):
    price_in_millions = property_dict['Price'] / 1000000
    sq_ft_in_thousands = property_dict['Square Footage'] / 1000

    features = property_dict['Features']
    if isinstance(features, list):
        features = ', '.join(features)

    property_text = f"Price: ${price_in_millions:.2f}M, Bedrooms: {property_dict['Bedrooms']}, " \
                     f"Bathrooms: {property_dict['Bathrooms']}, " \
                     f"Square Footage: {sq_ft_in_thousands:.1f}K sq ft, " \
                     f"Lot Size: {property_dict['Lot Size (Acres)']:.2f} acres, " \
                     f"Features: {features}"

    return property_text

# Setup LoRA for parameter-efficient fine-tuning
def setup_lora_model(base_model_path="meta-llama/Llama-3.1-8B", hf_token=None):
    print(f"Loading base model: {base_model_path}...")

    from transformers import AutoTokenizer, AutoModelForCausalLM

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        token=hf_token,
        use_fast=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token



    # Configure 4-bit quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True
    )

    # Load the model with 4-bit quantization
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        quantization_config=bnb_config,
        device_map="auto",
        token=hf_token
    )

    # Define LoRA configuration
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,  # Lower rank for less memory
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj"]  # Optimize for memory: target fewer modules
    )

    # Get the PEFT model
    model = get_peft_model(model, peft_config)

    # Print trainable parameters
    print("Trainable parameters:")
    model.print_trainable_parameters()

    return model, tokenizer

# Main execution
def main(file_path, output_dir="./llama3_real_estate_model", batch_size=4, epochs=3, model_path="meta-llama/Llama-3.1-8B", hf_token=None):
    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Load data
    df = load_data(file_path)

    # Split the data
    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

    # Initialize model and tokenizer with LoRA for efficient fine-tuning
    model, tokenizer = setup_lora_model(model_path, hf_token)

    # Create datasets
    train_dataset = RealEstateLlamaDataset(train_df, tokenizer)
    val_dataset = RealEstateLlamaDataset(val_df, tokenizer)

    # Train model
    trained_model = train_llama_model(
        model,
        train_dataset,
        val_dataset,
        tokenizer,
        output_dir,
        epochs=epochs,
        batch_size=batch_size
    )

    # Create a small dataloader for generating samples
    sample_dataloader = DataLoader(val_dataset, batch_size=1)

    # Generate a sample
    generate_sample(trained_model, tokenizer, sample_dataloader)

    # Test with a new property
    test_property = {
        'Price': 450000,
        'Bedrooms': '3',
        'Bathrooms': '2',
        'Square Footage': 1800,
        'Lot Size (Acres)': 0.25,
        'Features': ['Hardwood floors', 'Updated kitchen', 'Finished basement', 'Deck']
    }

    property_text = format_property_input(test_property)
    description = generate_listing_description(trained_model, tokenizer, property_text)

    print("\nGenerated listing for test property:")
    print(description)


#hf_token = os.environ.get('HUGGINGFACE_TOKEN', 'COPY TOKEN HERE')

main(
    file_path=filename,
    output_dir="./llama3_real_estate_model",
    batch_size=1,
    epochs=2,
    model_path="meta-llama/Llama-3.1-8B",
    hf_token=hf_token
)

"""Code for preprocessing descriptions"""

def post_process_description(generated_text, property_info):
    #Replace any incorrect values in the generated text with accurate ones from property_info

        # Extract values from property_info
        price_match = re.search(r'Price: \$([0-9.]+)M', property_info)
        sqft_match = re.search(r'Square Footage: ([0-9.]+)K', property_info)
        lot_match = re.search(r'Lot Size: ([0-9.]+) acres', property_info)

        price = price_match.group(1) if price_match else None
        sqft = sqft_match.group(1) if sqft_match else None
        lot_size = lot_match.group(1) if lot_match else None

        # Replace values in generated text
        if price:
            generated_text = re.sub(r'\$[0-9.]+M', f'${price}M', generated_text)
            generated_text = re.sub(r'\$[0-9.]+ million', f'${price}M', generated_text)

        if sqft:
            generated_text = re.sub(r'[0-9.]+K sq ft', f'{sqft}K sq ft', generated_text)
            generated_text = re.sub(r'[0-9.]+ thousand square feet', f'{sqft}K sq ft', generated_text)

        if lot_size:
            generated_text = re.sub(r'[0-9.]+ acres', f'{lot_size} acres', generated_text)

        return generated_text

"""Test the model on 50 observations for later annotation"""

def generate_batch_descriptions(
    input_csv_path,
    output_csv_path,
    model_path="./llama3_real_estate_model",
    max_length=150,
    save_interval=5  # Save results every N properties to avoid losing progress
):
    """
    Memory-efficient generator of listing descriptions for properties in a CSV file.

    Args:
        input_csv_path (str): Path to the input CSV file containing property details
        output_csv_path (str): Path to save the output CSV with generated descriptions
        model_path (str): Path to the trained model
        max_length (int): Maximum length of generated descriptions
        save_interval (int): Save progress after processing this many properties
    """
    import pandas as pd
    import torch
    import os
    import gc
    from tqdm import tqdm
    from transformers import AutoTokenizer, LlamaForCausalLM, LlamaTokenizer
    from peft import PeftModel, PeftConfig, get_peft_model

    # Check if CUDA is available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using {device} device")

    # Load the input CSV first
    print(f"Loading data from {input_csv_path}...")
    df = pd.read_csv(input_csv_path)
    print(f"Loaded {len(df)} properties")

    # Add a column for the descriptions if it doesn't exist
    if 'Generated Listing Description' not in df.columns:
        df['Generated Listing Description'] = None

    # Check if we're resuming a previous run
    completed_count = df['Generated Listing Description'].notna().sum()
    if completed_count > 0:
        print(f"Resuming from property {completed_count}/{len(df)}")

    # Load tokenizer first (much lighter on memory)
    print(f"Loading tokenizer from {model_path}...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
    except:
        print("Failed to load tokenizer from model path, trying base Llama tokenizer...")
        try:
            # Try to find a PEFT config to get the base model path
            config_path = os.path.join(model_path, "adapter_config.json")
            if os.path.exists(config_path):
                import json
                with open(config_path, 'r') as f:
                    config_data = json.load(f)
                    base_model_path = config_data.get('base_model_name_or_path')
                    print(f"Found base model path: {base_model_path}")
                    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
            else:
                # Fallback to a generic Llama tokenizer
                tokenizer = LlamaTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
        except Exception as e:
            print(f"Error loading tokenizer: {e}")
            return None

    # Function to format property input
    def format_property_input(row):
        try:
            # Convert values to appropriate formats with error handling
            try:
                price_in_millions = float(row['Price']) / 1000000
            except (ValueError, TypeError):
                price_in_millions = 0.0

            try:
                sq_ft_in_thousands = float(row['Square Footage']) / 1000
            except (ValueError, TypeError):
                sq_ft_in_thousands = 0.0

            try:
                lot_size = float(row['Lot Size (Acres)'])
            except (ValueError, TypeError):
                lot_size = 0.0

            # Handle features
            features = row.get('Features', "None")
            if pd.isna(features):
                features = "None"
            elif isinstance(features, list):
                features = ', '.join(features)
            elif isinstance(features, str) and (features.startswith('[') or features.startswith('{')):
                try:
                    evaluated = eval(features)
                    if isinstance(evaluated, (list, dict)):
                        if isinstance(evaluated, dict):
                            features = ', '.join([f"{k}: {v}" for k, v in evaluated.items()])
                        else:
                            features = ', '.join(evaluated)
                except:
                    pass

            # Format the property text
            property_text = (
                f"Price: ${price_in_millions:.2f}M, "
                f"Bedrooms: {row.get('Bedrooms', '0')}, "
                f"Bathrooms: {row.get('Bathrooms', '0')}, "
                f"Square Footage: {sq_ft_in_thousands:.1f}K sq ft, "
                f"Lot Size: {lot_size:.2f} acres, "
                f"Features: {features}"
            )

            return property_text
        except Exception as e:
            print(f"Error formatting property: {e}")
            return f"Price: $0.00M, Bedrooms: 0, Bathrooms: 0, Square Footage: 0.0K sq ft, Lot Size: 0.00 acres, Features: None"

    # Function to load model with minimum memory usage
    def load_model():
        print("Loading model (this may take a while)...")

        # Force garbage collection before loading model
        gc.collect()
        torch.cuda.empty_cache()

        try:
            # First, determine if this is a PEFT model
            is_peft = False
            peft_config = None

            if os.path.exists(os.path.join(model_path, "adapter_config.json")):
                try:
                    peft_config = PeftConfig.from_pretrained(model_path)
                    is_peft = True
                    print(f"Found PEFT config with base model: {peft_config.base_model_name_or_path}")
                except:
                    is_peft = False

            if is_peft and peft_config:
                # Load base model with minimum memory settings
                from transformers import BitsAndBytesConfig

                # 4-bit quantization config
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,
                    llm_int8_enable_fp32_cpu_offload=True
                )

                # Load the base model with extreme memory optimization
                base_model = LlamaForCausalLM.from_pretrained(
                    peft_config.base_model_name_or_path,
                    quantization_config=bnb_config,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True,
                )

                # Load the PEFT adapter
                model = PeftModel.from_pretrained(
                    base_model,
                    model_path,
                    torch_dtype=torch.float16,
                )
            else:
                # Load regular model with minimum memory
                model = LlamaForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True,
                    device_map="auto"
                )

            model.eval()
            return model

        except Exception as e:
            print(f"Error loading model: {e}")
            return None

    # Function to generate a description for a property
    def generate_listing_description(model, property_info, max_tokens=max_length):
        try:
            # Extract key values
            price_str = property_info.split('Price: $')[1].split('M')[0]
            sqft_str = property_info.split('Square Footage: ')[1].split('K')[0]
            lot_str = property_info.split('Lot Size: ')[1].split(' acres')[0]

            # Prepare input text
            input_text = f"<s>[INST] Write a compelling and accurate real estate listing description for this property. You MUST include and EXACTLY match these numeric values without rounding or changing them: Price: ${price_str}M, Square Footage: {sqft_str}K sq ft, Lot Size: {lot_str} acres. \n\n{property_info} [/INST]"

            # Tokenize
            input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)

            # Generate text
            with torch.no_grad():
                output = model.generate(
                    input_ids=input_ids,
                    max_new_tokens=max_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.2,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                )

            # Decode and extract
            generated_text = tokenizer.decode(output[0], skip_special_tokens=False)

            if "[/INST]" in generated_text:
                description_part = generated_text.split("[/INST]")[1].split("</s>")[0].strip()
            else:
                description_part = generated_text.split("</s>")[0].strip()

            description_part = post_process_description(description_part, property_info)
            return description_part

        except Exception as e:
            print(f"Error generating description: {e}")
            return "Error generating description"

    # Process properties and generate descriptions
    print("Starting generation process...")

    # Keep track of which rows we've already processed
    processed_indices = df.index[df['Generated Listing Description'].notna()].tolist()
    to_process_indices = [i for i in df.index if i not in processed_indices]

    if len(to_process_indices) > 0:
        # Load model only if we have properties to process
        model = load_model()
        if model is None:
            print("Failed to load model. Exiting.")
            return None

        # Process properties in batches and save periodically
        try:
            for i, idx in enumerate(tqdm(to_process_indices)):
                row = df.iloc[idx]
                property_text = format_property_input(row)
                description = generate_listing_description(model, property_text)
                df.at[idx, 'Generated Listing Description'] = description

                # Save progress periodically
                if (i + 1) % save_interval == 0 or i == len(to_process_indices) - 1:
                    print(f"Saving progress ({i + 1}/{len(to_process_indices)} completed)...")
                    df.to_csv(output_csv_path, index=False)

                # Clean up memory after each generation
                torch.cuda.empty_cache()
                gc.collect()

        except Exception as e:
            print(f"Error during generation: {e}")
        finally:
            # Clean up model to free memory
            del model
            torch.cuda.empty_cache()
            gc.collect()

            # Final save
            print("Saving final results...")
            df.to_csv(output_csv_path, index=False)
    else:
        print("All properties already have descriptions. Nothing to do.")

    print("Process complete!")
    return df


# Memory-optimized function to load saved weights and run inference on a single property
def generate_single_description(
    property_dict,
    model_path="./llama3_real_estate_model",
    max_length=150
):
    """
    Generate a single listing description without loading the full model into memory.
    Useful for testing or individual properties.

    Args:
        property_dict: Dictionary containing property details
        model_path: Path to the saved model
        max_length: Maximum length of the generated description

    Returns:
        Generated description string
    """
    import torch
    import gc
    from transformers import AutoTokenizer, LlamaForCausalLM
    from peft import PeftModel, PeftConfig

    # Format the property dictionary into text
    price_in_millions = float(property_dict['Price']) / 1000000
    sq_ft_in_thousands = float(property_dict['Square Footage']) / 1000

    features = property_dict.get('Features', "None")
    if isinstance(features, list):
        features = ', '.join(features)

    property_text = (
        f"Price: ${price_in_millions:.2f}M, "
        f"Bedrooms: {property_dict['Bedrooms']}, "
        f"Bathrooms: {property_dict['Bathrooms']}, "
        f"Square Footage: {sq_ft_in_thousands:.1f}K sq ft, "
        f"Lot Size: {float(property_dict['Lot Size (Acres)']):.2f} acres, "
        f"Features: {features}"
    )

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Determine if it's a PEFT model
    is_peft = False
    try:
        peft_config = PeftConfig.from_pretrained(model_path)
        is_peft = True
    except:
        is_peft = False

    # Load model with memory optimization
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    if is_peft:
        # Load base model with 4-bit quantization
        from transformers import BitsAndBytesConfig

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )

        base_model = LlamaForCausalLM.from_pretrained(
            peft_config.base_model_name_or_path,
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.float16
        )

        model = PeftModel.from_pretrained(base_model, model_path)
    else:
        model = LlamaForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            torch_dtype=torch.float16
        )

    model.eval()

    # Generate description
    try:
        # Extract values for the prompt
        price_str = property_text.split('Price: $')[1].split('M')[0]
        sqft_str = property_text.split('Square Footage: ')[1].split('K')[0]
        lot_str = property_text.split('Lot Size: ')[1].split(' acres')[0]

        # Prepare input text
        input_text = f"<s>[INST] Write a compelling and accurate real estate listing description for this property. You MUST include and EXACTLY match these numeric values without rounding or changing them: Price: ${price_str}M, Square Footage: {sqft_str}K sq ft, Lot Size: {lot_str} acres. \n\n{property_text} [/INST]"

        input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)

        with torch.no_grad():
            output = model.generate(
                input_ids=input_ids,
                max_new_tokens=max_length,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.2,
                do_sample=True
            )

        generated_text = tokenizer.decode(output[0], skip_special_tokens=False)

        if "[/INST]" in generated_text:
            description = generated_text.split("[/INST]")[1].split("</s>")[0].strip()
        else:
            description = generated_text.split("</s>")[0].strip()

    except Exception as e:
        description = f"Error generating description: {e}"

    # Clean up
    del model
    torch.cuda.empty_cache()
    gc.collect()

    return description


# Example usage
if __name__ == "__main__":
    # Update these paths as needed
    input_csv = "/content/housing_data_simulation test copy.csv"  # Your input CSV file
    output_csv = "housing_data_with_descriptions.csv"  # Where to save the results
    model_dir = "./llama3_real_estate_model"  # Directory containing your trained model

    # Generate descriptions for all properties in the CSV
    generate_batch_descriptions(
        input_csv_path=input_csv,
        output_csv_path=output_csv,
        model_path=model_dir,
        max_length=150,
        save_interval=1  # Save after each property to prevent data loss
    )

    # Alternatively, generate a description for a single property
    """
    test_property = {
        'Price': 450000,
        'Bedrooms': '3',
        'Bathrooms': '2',
        'Square Footage': 1800,
        'Lot Size (Acres)': 0.25,
        'Features': ['Hardwood floors', 'Updated kitchen', 'Finished basement', 'Deck']
    }

    description = generate_single_description(
        test_property,
        model_path=model_dir
    )
    print(description)
    """
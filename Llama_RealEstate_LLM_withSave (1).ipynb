{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load the training data"
      ],
      "metadata": {
        "id": "9fIN3jb5bMKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "filename = \"housing_data_simulation.csv\"\n",
        "\n",
        "# Check if the file already exists\n",
        "if not os.path.exists(filename):\n",
        "    # If it doesn't exist, upload it\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]  # Get the filename of the uploaded file\n",
        "    print(f\"File '{filename}' uploaded successfully.\")\n",
        "else:\n",
        "    print(f\"File '{filename}' already exists. Skipping upload.\")"
      ],
      "metadata": {
        "id": "HYmsW9UZuSuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If necessary, install bitsandbytes"
      ],
      "metadata": {
        "id": "BilsUEsjbOsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IE8ivmlPX80T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "ZA8SfHNrbkLu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo1fRjQekjmk",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Your current token setup code is fine, just make sure it runs before your model code\n",
        "hf_token = userdata.get('llama3.1token')  # or use os.environ.get('HUGGINGFACE_TOKEN')\n",
        "os.environ['HUGGINGFACE_TOKEN'] = hf_token\n",
        "login(token=hf_token)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    LlamaTokenizer,\n",
        "    LlamaForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import bitsandbytes\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define a persistent storage location on your Google Drive\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/ml_models/llama3_real_estate_model\"\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Load and preprocess the data\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load and preprocess the dataset.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Convert categorical features to strings\n",
        "    df['Bedrooms'] = df['Bedrooms'].astype(str)\n",
        "    df['Bathrooms'] = df['Bathrooms'].astype(str)\n",
        "\n",
        "    # Normalize numerical features\n",
        "    df['Price'] = df['Price'] / 1000000  # Convert to millions\n",
        "    df['Square Footage'] = df['Square Footage'] / 1000  # Convert to thousands\n",
        "    df['Lot Size (Acres)'] = df['Lot Size (Acres)']  # Keep as is\n",
        "\n",
        "    # Process features list into a string\n",
        "    if isinstance(df['Features'].iloc[0], str):\n",
        "        # If features are stored as strings that look like lists, convert them\n",
        "        try:\n",
        "            df['Features'] = df['Features'].apply(eval).apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
        "        except:\n",
        "            # If eval fails, assume it's already a string\n",
        "            pass\n",
        "    elif isinstance(df['Features'].iloc[0], list):\n",
        "        df['Features'] = df['Features'].apply(lambda x: ', '.join(x))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Custom dataset class for Llama\n",
        "class RealEstateLlamaDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # Create input text with a clear instruction for the model\n",
        "        property_text = f\"Price: ${row['Price']:.6f}M, Bedrooms: {row['Bedrooms']}, Bathrooms: {row['Bathrooms']}, \" \\\n",
        "                         f\"Square Footage: {row['Square Footage']:.4f}K sq ft, Lot Size: {row['Lot Size (Acres)']:.3f} acres, \" \\\n",
        "                         f\"Features: {row['Features']}\"\n",
        "\n",
        "        # Create full prompt in the instruction format Llama understands\n",
        "        full_text = f\"<s>[INST] Write a compelling and accurate real estate listing description for this property. Make sure that the adjectives you use to describe the house are appropriate considering the features of the house such as Price, Square footage and lot size. For example, you would not want to use the word expansive to describe a 0.1 acre lot or the word cozy to describe a 5000 square foot home. Any mention of Price, Square Footage and/or Lot Size in the listing description should EXACTLY match these (input) numeric values without rounding or changing them: \\n\\n{property_text} [/INST] {row['Listing Description']}</s>\"\n",
        "\n",
        "        # Tokenize full text\n",
        "        encodings = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Create labels - set to -100 for the instruction part so it's not included in loss calculation\n",
        "        labels = encodings['input_ids'].clone()\n",
        "\n",
        "        # Find where the [/INST] token ends\n",
        "        inst_tokens = self.tokenizer.encode(\" [/INST]\", add_special_tokens=False)\n",
        "        inst_end_positions = []\n",
        "\n",
        "        for i in range(len(labels[0]) - len(inst_tokens) + 1):\n",
        "            if labels[0][i:i+len(inst_tokens)].tolist() == inst_tokens:\n",
        "                inst_end_positions.append(i + len(inst_tokens) - 1)\n",
        "\n",
        "        if inst_end_positions:\n",
        "            # Set all tokens before the end of [/INST] to -100\n",
        "            labels[0, :inst_end_positions[0]+1] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': encodings['input_ids'].squeeze(),\n",
        "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
        "            'labels': labels.squeeze(),\n",
        "            'raw_property': property_text,\n",
        "            'raw_description': row['Listing Description']\n",
        "        }\n",
        "\n",
        "# Training function using Trainer API\n",
        "def train_llama_model(model, train_dataset, val_dataset, tokenizer, output_dir, epochs=1, lr=2e-4, batch_size=1):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch\n",
        "        warmup_steps=50,  # Reduce warmup steps\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"steps\",  # Use eval_strategy instead of evaluation_strategy\n",
        "        eval_steps=100,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        save_total_limit=2,  # Keep only 2 checkpoints to save space\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"none\",\n",
        "        learning_rate=lr,\n",
        "        fp16=True,  # Use fp16 for training\n",
        "        optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
        "        max_grad_norm=0.3,  # Gradient clipping\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Training complete. Saving model...\")\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Generate a sample description\n",
        "def generate_sample(model, tokenizer, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    # Get a random sample\n",
        "    batch = next(iter(dataloader))\n",
        "    property_info = batch['raw_property'][0]\n",
        "    actual_description = batch['raw_description'][0]\n",
        "\n",
        "    print(\"\\nSample Property:\")\n",
        "    print(property_info)\n",
        "\n",
        "    print(\"\\nActual Description:\")\n",
        "    print(actual_description)\n",
        "\n",
        "    # Generate description\n",
        "    generated_text = generate_listing_description(model, tokenizer, property_info)\n",
        "\n",
        "    print(\"\\nGenerated Description:\")\n",
        "    print(generated_text)\n",
        "\n",
        "# Generate listing description\n",
        "def generate_listing_description(model, tokenizer, property_info, max_length=150):\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare input text with prompt in the instruction format\n",
        "    input_text = f\"<s>[INST] Write a compelling and accurate real estate listing description for this property. Make sure that the adjectives you use to describe the house are appropriate considering the features of the house such as Price, Square footage and lot size. For example, you would not want to use the word expansive to describe a 0.1 acre lot or the word cozy to describe a 5000 square foot home. Any mention of Price, Square Footage and/or Lot Size in the listing description should EXACTLY match these (input) numeric values without rounding or changing them: Price: ${property_info.split('Price: $')[1].split('M')[0]}M, Square Footage: {property_info.split('Square Footage: ')[1].split('K')[0]}K sq ft, Lot Size: {property_info.split('Lot Size: ')[1].split(' acres')[0]} acres. \\n\\n{property_info} [/INST]\"\n",
        "    tokenized_input = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_input[\"input_ids\"].to(device)\n",
        "    attention_mask = tokenized_input[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Generate text\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask = attention_mask,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=0.9,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decode the output\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
        "\n",
        "    description_part = generated_text\n",
        "\n",
        "    # First extract the part after [/INST] if present\n",
        "    if \"[/INST]\" in description_part:\n",
        "        description_part = description_part.split(\"[/INST]\")[1]\n",
        "\n",
        "    # Then remove any end tokens\n",
        "    end_tokens = [\"</s>\", \"<|end_of_text|>\"]\n",
        "    for token in end_tokens:\n",
        "        if token in description_part:\n",
        "            description_part = description_part.split(token)[0]\n",
        "\n",
        "    description_part = description_part.strip()\n",
        "\n",
        "    return description_part\n",
        "\n",
        "# Function to format a new property for prediction\n",
        "def format_property_input(property_dict):\n",
        "    price_in_millions = property_dict['Price'] / 1000000\n",
        "    sq_ft_in_thousands = property_dict['Square Footage'] / 1000\n",
        "\n",
        "    features = property_dict['Features']\n",
        "    if isinstance(features, list):\n",
        "        features = ', '.join(features)\n",
        "\n",
        "    property_text = f\"Price: ${price_in_millions:.6f}M, Bedrooms: {property_dict['Bedrooms']}, \" \\\n",
        "                     f\"Bathrooms: {property_dict['Bathrooms']}, \" \\\n",
        "                     f\"Square Footage: {sq_ft_in_thousands:.4f}K sq ft, \" \\\n",
        "                     f\"Lot Size: {property_dict['Lot Size (Acres)']:.3f} acres, \" \\\n",
        "                     f\"Features: {features}\"\n",
        "\n",
        "    return property_text\n",
        "\n",
        "# Setup LoRA for parameter-efficient fine-tuning\n",
        "def setup_lora_model(base_model_path=\"meta-llama/Llama-3.1-8B\", hf_token=None):\n",
        "    print(f\"Loading base model: {base_model_path}...\")\n",
        "\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        base_model_path,\n",
        "        token=hf_token,\n",
        "        use_fast=True\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "\n",
        "    # Configure 4-bit quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "\n",
        "    # Load the model with 4-bit quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_path,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        token=hf_token\n",
        "    )\n",
        "\n",
        "    # Define LoRA configuration\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        inference_mode=False,\n",
        "        r=8,  # Lower rank for less memory\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"]  # Optimize for memory: target fewer modules\n",
        "    )\n",
        "\n",
        "    # Get the PEFT model\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Print trainable parameters\n",
        "    print(\"Trainable parameters:\")\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Main execution\n",
        "def main(file_path, output_dir=\"./llama3_real_estate_model\", batch_size=4, epochs=3, model_path=\"meta-llama/Llama-3.1-8B\", hf_token=None):\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load data\n",
        "    df = load_data(file_path)\n",
        "\n",
        "    # Split the data\n",
        "    train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Initialize model and tokenizer with LoRA for efficient fine-tuning\n",
        "    model, tokenizer = setup_lora_model(model_path, hf_token)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = RealEstateLlamaDataset(train_df, tokenizer)\n",
        "    val_dataset = RealEstateLlamaDataset(val_df, tokenizer)\n",
        "\n",
        "    # Train model\n",
        "    trained_model = train_llama_model(\n",
        "        model,\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        output_dir,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Create a small dataloader for generating samples\n",
        "    sample_dataloader = DataLoader(val_dataset, batch_size=1)\n",
        "\n",
        "    # Generate a sample\n",
        "    generate_sample(trained_model, tokenizer, sample_dataloader)\n",
        "\n",
        "    # Test with a new property\n",
        "    test_property = {\n",
        "        'Price': 450000,\n",
        "        'Bedrooms': '3',\n",
        "        'Bathrooms': '2',\n",
        "        'Square Footage': 1800,\n",
        "        'Lot Size (Acres)': 0.25,\n",
        "        'Features': ['Hardwood floors', 'Updated kitchen', 'Finished basement', 'Deck']\n",
        "    }\n",
        "\n",
        "    property_text = format_property_input(test_property)\n",
        "    description = generate_listing_description(trained_model, tokenizer, property_text)\n",
        "\n",
        "    print(\"\\nGenerated listing for test property:\")\n",
        "    print(description)\n",
        "\n",
        "    # Save the trained model\n",
        "    print(\"Saving the fine-tuned model...\")\n",
        "\n",
        "    # Save the LoRA adapter weights\n",
        "    trained_model.save_pretrained(f\"{output_dir}/lora_adapter\")\n",
        "\n",
        "    # Save the tokenizer\n",
        "    tokenizer.save_pretrained(f\"{output_dir}/tokenizer\")\n",
        "\n",
        "    print(f\"Model and tokenizer saved to {output_dir}\")\n",
        "\n",
        "#hf_token = os.environ.get('HUGGINGFACE_TOKEN', 'COPY TOKEN HERE')\n",
        "\n",
        "main(\n",
        "    file_path=filename,\n",
        "    output_dir = MODEL_SAVE_PATH,\n",
        "    batch_size=1,\n",
        "    epochs=2,\n",
        "    model_path=\"meta-llama/Llama-3.1-8B\",\n",
        "    hf_token=hf_token\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Saved Model and Test"
      ],
      "metadata": {
        "id": "7L6V0imNEmR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Your current token setup code is fine, just make sure it runs before your model code\n",
        "hf_token = userdata.get('llama3.1token')  # or use os.environ.get('HUGGINGFACE_TOKEN')\n",
        "os.environ['HUGGINGFACE_TOKEN'] = hf_token\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "SsW35r7yUNzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def format_property_input(property_dict):\n",
        "    price_in_millions = property_dict['Price'] / 1000000\n",
        "    sq_ft_in_thousands = property_dict['Square Footage'] / 1000\n",
        "\n",
        "    features = property_dict['Features']\n",
        "    if isinstance(features, list):\n",
        "        features = ', '.join(features)\n",
        "\n",
        "    property_text = f\"Price: ${price_in_millions:.6f}M, Bedrooms: {property_dict['Bedrooms']}, \" \\\n",
        "                     f\"Bathrooms: {property_dict['Bathrooms']}, \" \\\n",
        "                     f\"Square Footage: {sq_ft_in_thousands:.4f}K sq ft, \" \\\n",
        "                     f\"Lot Size: {property_dict['Lot Size (Acres)']:.3f} acres, \" \\\n",
        "                     f\"Features: {features}\"\n",
        "\n",
        "    return property_text\n",
        "\n",
        "def load_saved_model(adapter_path, base_model_path=\"meta-llama/Llama-3.1-8B\", hf_token=None):\n",
        "    \"\"\"\n",
        "    Load a previously saved LoRA model for inference with memory optimizations\n",
        "    \"\"\"\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "    from peft import PeftModel\n",
        "    import torch\n",
        "    import gc\n",
        "\n",
        "    # First, clear CUDA cache and run garbage collection\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Loading tokenizer from {adapter_path}/tokenizer...\")\n",
        "    # Load the saved tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        f\"{adapter_path}/tokenizer\",\n",
        "        token=hf_token\n",
        "    )\n",
        "\n",
        "    # Configure 8-bit quantization (uses less memory than 4-bit for inference)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,  # Use 8-bit instead of 4-bit\n",
        "        llm_int8_threshold=6.0,\n",
        "        llm_int8_skip_modules=None,\n",
        "        llm_int8_enable_fp32_cpu_offload=True\n",
        "    )\n",
        "\n",
        "    print(f\"Loading base model: {base_model_path}...\")\n",
        "    # Load the base model with 8-bit quantization\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_path,\n",
        "        quantization_config=bnb_config,\n",
        "        token=hf_token,\n",
        "        device_map=\"auto\",  # Let the library decide optimal placement\n",
        "        torch_dtype=torch.float16,  # Use float16 for weights\n",
        "        offload_folder=\"offload_folder\",  # Enable disk offloading\n",
        "        offload_state_dict=True,  # Offload state dict to disk during loading\n",
        "        low_cpu_mem_usage=True    # Optimize CPU memory usage\n",
        "    )\n",
        "\n",
        "    print(f\"Loading LoRA adapter from {adapter_path}/lora_adapter...\")\n",
        "    # Load the LoRA adapter weights\n",
        "    model = PeftModel.from_pretrained(base_model, f\"{adapter_path}/lora_adapter\")\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Model loaded successfully!\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# Modify generate_with_saved_model to use more memory-efficient generation\n",
        "def generate_with_saved_model(output_dir, property_info):\n",
        "    \"\"\"\n",
        "    Generate a real estate listing using a saved model with memory optimizations\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    import gc\n",
        "\n",
        "    # Clear memory before loading model\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Load the saved model\n",
        "    model, tokenizer = load_saved_model(\n",
        "        adapter_path=output_dir,\n",
        "        base_model_path=\"meta-llama/Llama-3.1-8B\",\n",
        "        hf_token=hf_token\n",
        "    )\n",
        "\n",
        "    # Generate with more memory-efficient settings\n",
        "    description = generate_listing_description_memory_efficient(model, tokenizer, property_info)\n",
        "\n",
        "    print(\"\\nGenerated listing:\")\n",
        "    print(description)\n",
        "\n",
        "    # Clean up memory after generation\n",
        "    del model\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return description\n",
        "\n",
        "# More memory-efficient generation function\n",
        "def generate_listing_description_memory_efficient(model, tokenizer, property_info, max_length=150):\n",
        "    import torch\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare input text with prompt in the instruction format\n",
        "    input_text = f\"<s>[INST] Write a compelling and accurate real estate listing description for this property. Make sure that the adjectives you use to describe the house are appropriate considering the features of the house such as Price, Square footage and lot size. For example, you would not want to use the word expansive to describe a 0.1 acre lot or the word cozy to describe a 5000 square foot home. Any mention of Price, Square Footage and/or Lot Size in the listing description should EXACTLY match these (input) numeric values without rounding or changing them: Price: ${property_info.split('Price: $')[1].split('M')[0]}M, Square Footage: {property_info.split('Square Footage: ')[1].split('K')[0]}K sq ft, Lot Size: {property_info.split('Lot Size: ')[1].split(' acres')[0]} acres. \\n\\n{property_info} [/INST]\"\n",
        "\n",
        "    tokenized_input = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_input[\"input_ids\"].to(model.device)\n",
        "    attention_mask = tokenized_input[\"attention_mask\"].to(model.device)\n",
        "\n",
        "    # Generate text with memory-efficient settings\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=0.7,  # Lower temperature for more deterministic output (uses less memory)\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            # Memory efficient settings:\n",
        "            use_cache=True,        # Enable KV caching\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "    # Decode the output\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
        "\n",
        "    # Process output to extract just the description\n",
        "    if \"[/INST]\" in generated_text:\n",
        "        description_part = generated_text.split(\"[/INST]\")[1]\n",
        "    else:\n",
        "        description_part = generated_text\n",
        "\n",
        "    # Remove any end tokens\n",
        "    end_tokens = [\"</s>\", \"<|end_of_text|>\"]\n",
        "    for token in end_tokens:\n",
        "        if token in description_part:\n",
        "            description_part = description_part.split(token)[0]\n",
        "\n",
        "    return description_part.strip()\n",
        "\n",
        "# Code to test the model with the optimized functions\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/ml_models/llama3_real_estate_model\"\n",
        "\n",
        "# Test with a new property\n",
        "test_property = {\n",
        "    'Price': 650000,\n",
        "    'Bedrooms': '4',\n",
        "    'Bathrooms': '2.5',\n",
        "    'Square Footage': 2200,\n",
        "    'Lot Size (Acres)': 0.32,\n",
        "    'Features': ['New roof', 'Stainless appliances', 'Fenced yard', 'Fireplace']\n",
        "}\n",
        "\n",
        "# Format the property info\n",
        "property_text = format_property_input(test_property)\n",
        "\n",
        "# Generate using the saved model with memory optimizations\n",
        "generate_with_saved_model(MODEL_SAVE_PATH, property_text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QAMQ_nLTRff6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.save()"
      ],
      "metadata": {
        "id": "j8zeL2UcT2Iq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import test file"
      ],
      "metadata": {
        "id": "NP8Z_gjuktlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "filename = \"housing_data_simulation test copy.csv\"\n",
        "\n",
        "# Check if the file already exists\n",
        "if not os.path.exists(filename):\n",
        "    # If it doesn't exist, upload it\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]  # Get the filename of the uploaded file\n",
        "    print(f\"File '{filename}' uploaded successfully.\")\n",
        "else:\n",
        "    print(f\"File '{filename}' already exists. Skipping upload.\")"
      ],
      "metadata": {
        "id": "Qz67bU8bVjRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model on 50 observations for later annotation"
      ],
      "metadata": {
        "id": "1UQ5Gq36G2Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def post_process_description(generated_text, property_info):\n",
        "    \"\"\"\n",
        "    Ensures that numerical values in the generated text match the desired format:\n",
        "    - Price: $300,000 (full dollar amount with commas)\n",
        "    - Square footage: 1234 (whole number)\n",
        "    - Acres: 0.15 (decimal number)\n",
        "\n",
        "    Args:\n",
        "        generated_text (str): The generated listing description\n",
        "        property_info (str): The original property information string\n",
        "\n",
        "    Returns:\n",
        "        str: The processed description with correctly formatted numerical values\n",
        "    \"\"\"\n",
        "    # Extract the original values from property_info\n",
        "    price_match = re.search(r'Price: \\$([\\d.]+)M', property_info)\n",
        "    sqft_match = re.search(r'Square Footage: ([\\d.]+)K', property_info)\n",
        "    lot_match = re.search(r'Lot Size: ([\\d.]+) acres', property_info)\n",
        "\n",
        "    # Convert to the desired formats\n",
        "    if price_match:\n",
        "        # Convert $X.YM to $X,YY0,000\n",
        "        price_in_millions = float(price_match.group(1))\n",
        "        price_in_dollars = int(price_in_millions * 1000000)\n",
        "        formatted_price = \"${:,}\".format(price_in_dollars)\n",
        "    else:\n",
        "        formatted_price = None\n",
        "\n",
        "    if sqft_match:\n",
        "        # Convert X.YK to whole number\n",
        "        sqft_in_thousands = float(sqft_match.group(1))\n",
        "        formatted_sqft = str(int(sqft_in_thousands * 1000))\n",
        "    else:\n",
        "        formatted_sqft = None\n",
        "\n",
        "    if lot_match:\n",
        "        # Keep exact decimal format for acreage\n",
        "        formatted_lot = lot_match.group(1)\n",
        "    else:\n",
        "        formatted_lot = None\n",
        "\n",
        "    # Only proceed with replacements if we have valid values\n",
        "    if not (formatted_price or formatted_sqft or formatted_lot):\n",
        "        print(\"Warning: Could not extract values from property_info\")\n",
        "        return generated_text\n",
        "\n",
        "    processed_text = generated_text\n",
        "\n",
        "    # Replace price variations\n",
        "    if formatted_price:\n",
        "        # Handle $X.YM format\n",
        "        processed_text = re.sub(r'\\$([\\d,.]+)M', formatted_price, processed_text)\n",
        "        processed_text = re.sub(r'\\$([\\d,.]+) million', formatted_price, processed_text)\n",
        "        processed_text = re.sub(r'\\$([\\d,.]+) Million', formatted_price, processed_text)\n",
        "\n",
        "        # Handle normal dollar amounts\n",
        "        processed_text = re.sub(r'\\$([\\d,]+)', formatted_price, processed_text)\n",
        "\n",
        "        # Handle spelled out numbers\n",
        "        million_words = ['million', 'Million']\n",
        "        for word in million_words:\n",
        "            pattern = rf'(\\$[\\d,.]+ {word}|\\$[\\d,.]+{word})'\n",
        "            processed_text = re.sub(pattern, formatted_price, processed_text)\n",
        "\n",
        "    # Replace square footage variations\n",
        "    if formatted_sqft:\n",
        "        # Handle XK sq ft format\n",
        "        processed_text = re.sub(r'([\\d,.]+)K sq ft', formatted_sqft + \" sq ft\", processed_text)\n",
        "        processed_text = re.sub(r'([\\d,.]+)K square feet', formatted_sqft + \" square feet\", processed_text)\n",
        "\n",
        "        # Handle normal square footage formats\n",
        "        processed_text = re.sub(r'([\\d,.]+) sq\\. ft\\.', formatted_sqft + \" sq. ft.\", processed_text)\n",
        "        processed_text = re.sub(r'([\\d,.]+) square feet', formatted_sqft + \" square feet\", processed_text)\n",
        "        processed_text = re.sub(r'([\\d,.]+) sf', formatted_sqft + \" sq ft\", processed_text)\n",
        "\n",
        "        # Handle thousand variations\n",
        "        thousand_words = ['thousand square feet', 'thousand sq ft', 'thousand sq. ft.']\n",
        "        for word in thousand_words:\n",
        "            pattern = rf'([\\d,.]+) {word}'\n",
        "            processed_text = re.sub(pattern, formatted_sqft + \" square feet\", processed_text)\n",
        "\n",
        "    # Replace lot size variations\n",
        "    if formatted_lot:\n",
        "        # Standard lot size formats\n",
        "        processed_text = re.sub(r'([\\d,.]+) acre(?!s)', formatted_lot + \" acre\", processed_text)\n",
        "        processed_text = re.sub(r'([\\d,.]+) acres', formatted_lot + \" acres\", processed_text)\n",
        "        processed_text = re.sub(r'([\\d,.]+)-acre', formatted_lot + \"-acre\", processed_text)\n",
        "\n",
        "    # Check if values were properly inserted and add them if missing\n",
        "    if formatted_price and formatted_price not in processed_text:\n",
        "        processed_text = f\"Priced at {formatted_price}. \" + processed_text\n",
        "\n",
        "    if formatted_sqft and formatted_sqft + \" sq\" not in processed_text:\n",
        "        processed_text = f\"This {formatted_sqft} sq ft home \" + processed_text\n",
        "\n",
        "    if formatted_lot and formatted_lot + \" acre\" not in processed_text:\n",
        "        if \"acre\" in processed_text:\n",
        "            # Try to insert near an existing mention of acres\n",
        "            processed_text = re.sub(r'([\\d,.]+) acres?', f\"{formatted_lot} acres\", processed_text, count=1)\n",
        "        else:\n",
        "            # Add to beginning if no mention exists\n",
        "            processed_text = f\"Situated on {formatted_lot} acres. \" + processed_text\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "def generate_batch_descriptions(\n",
        "    input_csv_path,\n",
        "    output_csv_path,\n",
        "    model_path=\"./llama3_real_estate_model\",\n",
        "    max_length=150,\n",
        "    save_interval=5  # Save results every N properties to avoid losing progress\n",
        "):\n",
        "    \"\"\"\n",
        "    Memory-efficient generator of listing descriptions for properties in a CSV file.\n",
        "\n",
        "    Args:\n",
        "        input_csv_path (str): Path to the input CSV file containing property details\n",
        "        output_csv_path (str): Path to save the output CSV with generated descriptions\n",
        "        model_path (str): Path to the trained model\n",
        "        max_length (int): Maximum length of generated descriptions\n",
        "        save_interval (int): Save progress after processing this many properties\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import torch\n",
        "    import os\n",
        "    import gc\n",
        "    from tqdm import tqdm\n",
        "    from transformers import AutoTokenizer, LlamaForCausalLM, LlamaTokenizer\n",
        "    from peft import PeftModel, PeftConfig, get_peft_model\n",
        "\n",
        "    # Check if CUDA is available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using {device} device\")\n",
        "\n",
        "    # Load the input CSV first\n",
        "    print(f\"Loading data from {input_csv_path}...\")\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "    print(f\"Loaded {len(df)} properties\")\n",
        "\n",
        "    # Add a column for the descriptions if it doesn't exist\n",
        "    if 'Generated Listing Description' not in df.columns:\n",
        "        df['Generated Listing Description'] = None\n",
        "\n",
        "    # Check if we're resuming a previous run\n",
        "    completed_count = df['Generated Listing Description'].notna().sum()\n",
        "    if completed_count > 0:\n",
        "        print(f\"Resuming from property {completed_count}/{len(df)}\")\n",
        "\n",
        "    # Load tokenizer first (much lighter on memory)\n",
        "    print(f\"Loading tokenizer from {model_path}...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "    except:\n",
        "        print(\"Failed to load tokenizer from model path, trying base Llama tokenizer...\")\n",
        "        try:\n",
        "            # Try to find a PEFT config to get the base model path\n",
        "            config_path = os.path.join(model_path, \"adapter_config.json\")\n",
        "            if os.path.exists(config_path):\n",
        "                import json\n",
        "                with open(config_path, 'r') as f:\n",
        "                    config_data = json.load(f)\n",
        "                    base_model_path = config_data.get('base_model_name_or_path')\n",
        "                    print(f\"Found base model path: {base_model_path}\")\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "            else:\n",
        "                # Fallback to a generic Llama tokenizer\n",
        "                tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading tokenizer: {e}\")\n",
        "            return None\n",
        "\n",
        "    def format_property_input(row):\n",
        "        try:\n",
        "            # Convert values to appropriate formats with error handling\n",
        "            try:\n",
        "                price_in_millions = float(row['Price']) / 1000000\n",
        "                # We'll keep the price in millions format for the input\n",
        "            except (ValueError, TypeError):\n",
        "                price_in_millions = 0.0\n",
        "\n",
        "            try:\n",
        "                sq_ft_in_thousands = float(row['Square Footage']) / 1000\n",
        "                # We'll keep the square footage in thousands format for the input\n",
        "            except (ValueError, TypeError):\n",
        "                sq_ft_in_thousands = 0.0\n",
        "\n",
        "            try:\n",
        "                lot_size = float(row['Lot Size (Acres)'])\n",
        "            except (ValueError, TypeError):\n",
        "                lot_size = 0.0\n",
        "\n",
        "            # Handle features\n",
        "            features = row.get('Features', \"None\")\n",
        "            if pd.isna(features):\n",
        "                features = \"None\"\n",
        "            elif isinstance(features, list):\n",
        "                features = ', '.join(features)\n",
        "            elif isinstance(features, str) and (features.startswith('[') or features.startswith('{')):\n",
        "                try:\n",
        "                    evaluated = eval(features)\n",
        "                    if isinstance(evaluated, (list, dict)):\n",
        "                        if isinstance(evaluated, dict):\n",
        "                            features = ', '.join([f\"{k}: {v}\" for k, v in evaluated.items()])\n",
        "                        else:\n",
        "                            features = ', '.join(evaluated)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Format the property text - keeping the original format for internal processing\n",
        "            property_text = (\n",
        "                f\"Price: ${price_in_millions:.6f}M, \"\n",
        "                f\"Bedrooms: {row.get('Bedrooms', '0')}, \"\n",
        "                f\"Bathrooms: {row.get('Bathrooms', '0')}, \"\n",
        "                f\"Square Footage: {sq_ft_in_thousands:.4f}K sq ft, \"\n",
        "                f\"Lot Size: {lot_size:.2f} acres, \"\n",
        "                f\"Features: {features}\"\n",
        "            )\n",
        "\n",
        "            return property_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error formatting property: {e}\")\n",
        "            return f\"Price: $0.00M, Bedrooms: 0, Bathrooms: 0, Square Footage: 0.0K sq ft, Lot Size: 0.00 acres, Features: None\"\n",
        "\n",
        "    # Function to load model with minimum memory usage\n",
        "    def load_model():\n",
        "        print(\"Loading model (this may take a while)...\")\n",
        "\n",
        "        # Force garbage collection before loading model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        try:\n",
        "            # First, determine if this is a PEFT model\n",
        "            is_peft = False\n",
        "            peft_config = None\n",
        "\n",
        "            if os.path.exists(os.path.join(model_path, \"adapter_config.json\")):\n",
        "                try:\n",
        "                    peft_config = PeftConfig.from_pretrained(model_path)\n",
        "                    is_peft = True\n",
        "                    print(f\"Found PEFT config with base model: {peft_config.base_model_name_or_path}\")\n",
        "                except:\n",
        "                    is_peft = False\n",
        "\n",
        "            if is_peft and peft_config:\n",
        "                # Load base model with minimum memory settings\n",
        "                from transformers import BitsAndBytesConfig\n",
        "\n",
        "                # 4-bit quantization config\n",
        "                bnb_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.float16,\n",
        "                    llm_int8_enable_fp32_cpu_offload=True\n",
        "                )\n",
        "\n",
        "                # Load the base model with extreme memory optimization\n",
        "                base_model = LlamaForCausalLM.from_pretrained(\n",
        "                    peft_config.base_model_name_or_path,\n",
        "                    quantization_config=bnb_config,\n",
        "                    device_map=\"auto\",\n",
        "                    torch_dtype=torch.float16,\n",
        "                    low_cpu_mem_usage=True,\n",
        "                )\n",
        "\n",
        "                # Load the PEFT adapter\n",
        "                model = PeftModel.from_pretrained(\n",
        "                    base_model,\n",
        "                    model_path,\n",
        "                    torch_dtype=torch.float16,\n",
        "                )\n",
        "            else:\n",
        "                # Load regular model with minimum memory\n",
        "                model = LlamaForCausalLM.from_pretrained(\n",
        "                    model_path,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    low_cpu_mem_usage=True,\n",
        "                    device_map=\"auto\"\n",
        "                )\n",
        "\n",
        "            model.eval()\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_listing_description(model, property_info, max_tokens=max_length):\n",
        "        try:\n",
        "            # Extract key values\n",
        "            price_match = re.search(r'Price: \\$([\\d.]+)M', property_info)\n",
        "            sqft_match = re.search(r'Square Footage: ([\\d.]+)K', property_info)\n",
        "            lot_match = re.search(r'Lot Size: ([\\d.]+) acres', property_info)\n",
        "\n",
        "            # Convert to desired formats for the prompt\n",
        "            if price_match:\n",
        "                price_in_millions = float(price_match.group(1))\n",
        "                price_in_dollars = int(price_in_millions * 1000000)\n",
        "                formatted_price = \"${:,}\".format(price_in_dollars)\n",
        "            else:\n",
        "                formatted_price = \"unknown price\"\n",
        "\n",
        "            if sqft_match:\n",
        "                sqft_in_thousands = float(sqft_match.group(1))\n",
        "                formatted_sqft = str(int(sqft_in_thousands * 1000))\n",
        "            else:\n",
        "                formatted_sqft = \"unknown square footage\"\n",
        "\n",
        "            if lot_match:\n",
        "                formatted_lot = lot_match.group(1)\n",
        "            else:\n",
        "                formatted_lot = \"unknown acreage\"\n",
        "\n",
        "            # Prepare input text with exact values\n",
        "            input_text = f\"<s>[INST] Write a compelling, accurate, and unique real estate listing description for this property. You MUST include and EXACTLY match these numeric values without rounding or changing them: Price: {formatted_price}, Square Footage: {formatted_sqft} sq ft, Lot Size: {formatted_lot} acres. \\n\\n{property_info} [/INST]\"\n",
        "\n",
        "            # Tokenize\n",
        "            input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            # Generate text\n",
        "            with torch.no_grad():\n",
        "                output = model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    max_new_tokens=max_tokens,\n",
        "                    temperature=0.9,\n",
        "                    top_p=0.95,\n",
        "                    repetition_penalty=1.2,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id,\n",
        "                )\n",
        "\n",
        "            # Decode and extract\n",
        "            generated_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
        "\n",
        "            # Handle multiple possible end token formats\n",
        "            description_part = generated_text\n",
        "\n",
        "            # First extract the part after [/INST] if present\n",
        "            if \"[/INST]\" in description_part:\n",
        "                description_part = description_part.split(\"[/INST]\")[1]\n",
        "\n",
        "            # Then remove any end tokens\n",
        "            end_tokens = [\"</s>\", \"<|end_of_text|>\"]\n",
        "            for token in end_tokens:\n",
        "                if token in description_part:\n",
        "                    description_part = description_part.split(token)[0]\n",
        "\n",
        "            description_part = description_part.strip()\n",
        "\n",
        "            # Apply post-processing to ensure values match exactly\n",
        "            description_part = post_process_description(description_part, property_info)\n",
        "            return description_part\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating description: {e}\")\n",
        "            return \"Error generating description\"\n",
        "\n",
        "    # Process properties and generate descriptions\n",
        "    print(\"Starting generation process...\")\n",
        "\n",
        "    # Keep track of which rows we've already processed\n",
        "    processed_indices = df.index[df['Generated Listing Description'].notna()].tolist()\n",
        "    to_process_indices = [i for i in df.index if i not in processed_indices]\n",
        "\n",
        "    if len(to_process_indices) > 0:\n",
        "        # Load model only if we have properties to process\n",
        "        model = load_model()\n",
        "        if model is None:\n",
        "            print(\"Failed to load model. Exiting.\")\n",
        "            return None\n",
        "\n",
        "        # Process properties in batches and save periodically\n",
        "        try:\n",
        "            for i, idx in enumerate(tqdm(to_process_indices)):\n",
        "                row = df.iloc[idx]\n",
        "                property_text = format_property_input(row)\n",
        "                description = generate_listing_description(model, property_text)\n",
        "                df.at[idx, 'Generated Listing Description'] = description\n",
        "\n",
        "                # Save progress periodically\n",
        "                if (i + 1) % save_interval == 0 or i == len(to_process_indices) - 1:\n",
        "                    print(f\"Saving progress ({i + 1}/{len(to_process_indices)} completed)...\")\n",
        "                    df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "                # Clean up memory after each generation\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "        finally:\n",
        "            # Clean up model to free memory\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            # Final save\n",
        "            print(\"Saving final results...\")\n",
        "            df.to_csv(output_csv_path, index=False)\n",
        "    else:\n",
        "        print(\"All properties already have descriptions. Nothing to do.\")\n",
        "\n",
        "    print(\"Process complete!\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "0uoVvKVJQ3KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Update these paths as needed\n",
        "    input_csv = \"/content/housing_data_simulation test copy.csv\"  # Your input CSV file\n",
        "    output_csv = \"housing_data_with_descriptions.csv\"  # Where to save the results\n",
        "    model_dir = \"/content/drive/MyDrive/ml_models/llama3_real_estate_model\"  # Directory containing your trained model\n",
        "\n",
        "    # Generate descriptions for all properties in the CSV\n",
        "    generate_batch_descriptions(\n",
        "        input_csv_path=input_csv,\n",
        "        output_csv_path=output_csv,\n",
        "        model_path=model_dir,\n",
        "        max_length=150,\n",
        "        save_interval=5  # Save after each property to prevent data loss\n",
        "    )\n"
      ],
      "metadata": {
        "id": "3pdBqa_BDc4J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}